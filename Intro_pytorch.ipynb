{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Introduction to Pytorch & Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Outline\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Chapter-Learning-Objectives\" data-toc-modified-id=\"Chapter-Learning-Objectives-2\">Chapter Learning Objectives</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-3\">Imports</a></span></li><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-4\">1. Introduction</a></span></li><li><span><a href=\"#2.-PyTorch's-Tensor\" data-toc-modified-id=\"2.-PyTorch's-Tensor-5\">2. PyTorch's Tensor</a></span></li><li><span><a href=\"#3.-Neural-Network-Basics\" data-toc-modified-id=\"3.-Neural-Network-Basics-6\">3. Neural Network Basics</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter Learning Objectives\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe the difference between `NumPy` and `torch` arrays (`np.array` vs. `torch.Tensor`).\n",
    "- Explain fundamental concepts of neural networks such as layers, nodes, activation functions, etc.\n",
    "- Create a simple neural network in PyTorch for regression or classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_regression, make_circles, make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is a Python-based tool for scientific computing that provides several main features:\n",
    "- `torch.Tensor`, an n-dimensional array similar to that of `NumPy`, but which can run on GPUs\n",
    "- Computational graphs and an automatic differentiation enginge for building and training neural networks\n",
    "\n",
    "You can install PyTorch from: https://pytorch.org/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch's Tensor\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch a tensor is just like NumPy's `ndarray`.\n",
    "\n",
    "A key difference between PyTorch's `torch.Tensor` and NumPy's `np.array` is that `torch.Tensor` was constructed to integrate with GPUs and PyTorch's computational graphs (more on that next chapter though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. `ndarray` vs `tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating and working with tensors is much the same as with NumPy `ndarrays`. You can create a tensor with `torch.tensor()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = torch.tensor([1, 2, 3])\n",
    "tensor_2 = torch.tensor([1, 2, 3], dtype=torch.float32)\n",
    "tensor_3 = torch.tensor(np.array([1, 2, 3]))\n",
    "\n",
    "for t in [tensor_1, tensor_2, tensor_3]:\n",
    "    print(f\"{t}, dtype: {t.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also comes with most of the `NumPy` functions you're probably already familiar with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(2, 2)  # zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones(2, 2)  # ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(3, 2)  # random normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(4, 2, 3)  # rand uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in NumPy we can look at the shape of a tensor with the `.shape` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(2, 3, 2, 2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Tensors and Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different data types have different memory and computational implications. In Pytorch we'll be building networks that require thousands or even millions of floating point calculations! In such cases, using a smaller dtype like `float32` can significantly speed up computations and reduce memory requirements. The default float dtype in pytorch `float32`, as opposed to NumPy's `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array([3.14159]).dtype)\n",
    "print(torch.tensor([3.14159]).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But just like in NumPy, you can always specify the particular dtype you want using the `dtype` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.tensor([3.14159], dtype=torch.float64).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors operate just like `ndarrays` and have a variety of familiar methods that can be called off them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1, 3)\n",
    "b = torch.rand(3, 1)\n",
    "\n",
    "a + b  # broadcasting betweean a 1 x 3 and 3 x 1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a * b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, same as NumPy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(5, 2)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[0, :])\n",
    "print(X[0])\n",
    "print(X[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. GPU and CUDA Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU stands for \"graphical processing unit\" (as opposed to a CPU: central processing unit). GPUs were originally developed for gaming, they are very fast at performing operations on large amounts of data by performing them in parallel (think about updating the value of all pixels on a screen very quickly as a player moves around in a game). More recently, GPUs have been adapted for more general purpose programming. Neural networks can typically be broken into smaller computations that can be performed in parallel on a GPU. PyTorch is tightly integrated with CUDA - a software layer that facilitates interactions with a GPU (if you have one). You can check if you have GPU capability using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()  # my MacBook Pro does not have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on a machine that has a GPU, you need to tell PyTorch you want to use it. You'll see the following at the top of most PyTorch code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then use the `device` argument when creating tensors to specify whether you wish to use a CPU or GPU. Or if you want to move a tensor between the CPU and GPU, you can use the `.to()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 2, 2, device=device)\n",
    "print(X.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.to('cuda')  # this would give me an error as I don't have a GPU so I'm commenting out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll revisit GPUs later in the course when we are working with bigger datasets and more complex networks. For now, we can work on the CPU just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Network Basics\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's probably that you've already learned about several machine learning algorithms (kNN, Random Forest, SVM, etc.). Neural networks are simply another algorithm and actually one of the simplest in my opinion! As we'll see, a neural network is just a sequence of linear and non-linear transformations. Often you see something like this when learning about/using neural networks:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-6.png)\n",
    "\n",
    "So what on Earth does that all mean? Well we are going to build up some intuition one step at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Simple Linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple regression dataset with 500 observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=500, n_features=1, random_state=0, noise=10.0)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot_regression(X, y)\n",
    "# Plot the regression data\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.5, label='Data Points')\n",
    "plt.title('Regression Data', fontsize=14)\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fit a simple linear regression to this data using sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LinearRegression().fit(X, y)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.5, label='Data Points')\n",
    "\n",
    "plt.plot(X, sk_model.predict(X), color='red', label='Fitted line')\n",
    "plt.title('Regression Data', fontsize=14)\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_regression(X, y, sk_model.predict(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the parameters of that fitted line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"w_0: {sk_model.intercept_:.2f} (bias/intercept)\")\n",
    "print(f\"w_1: {sk_model.coef_[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an equation, that looks like this:\n",
    "\n",
    "$$\\hat{y}=-0.77 + 45.50X$$\n",
    "\n",
    "Or in matrix form:\n",
    "\n",
    "$$\\begin{bmatrix} \\hat{y_1} \\\\ \\hat{y_2} \\\\ \\vdots \\\\ \\hat{y_n} \\end{bmatrix}=\\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} -0.77 \\\\ 45.55 \\end{bmatrix}$$\n",
    "\n",
    "Or in graph form I'll represent it like this: \n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Linear Regression with a Neural Network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's implement the above in PyTorch to start gaining an intuition about neural networks! Almost every neural network model you build in PyTorch will inherit from `torch.nn.Module`. \n",
    "\n",
    "Let's create a model called `linearRegression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearRegression(nn.Module):  # our class inherits from nn.Module and we can call it anything we like\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()                                # super().__init__() makes our class inherit everything from torch.nn.Module\n",
    "        self.linear = nn.Linear(input_size, output_size)  # this is a simple linear layer: wX + b\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's step through the above:\n",
    "\n",
    "```python\n",
    "class linearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__() \n",
    "```\n",
    "\n",
    "^ Here we're creating a class called `linearRegression` and inheriting the methods and attributes of `nn.Module` (hint: try typing `help(linearRegression)` to see all the things we inheritied from `nn.Module`).\n",
    "\n",
    "```python\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "```\n",
    "\n",
    "^ Here we're defining a \"Linear\" layer, which just means `wX + b`, i.e., the weights of the network, multiplied by the input features plus the bias.\n",
    "\n",
    "```python\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "```\n",
    "\n",
    "^ PyTorch networks created with `nn.Module` must have a `forward()` method. It accepts the input data `x` and passes it through the defined operations. In this case, we are passing `x` into our linear layer and getting an output `out`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the model class, we can create an instance of that class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearRegression(input_size=1, output_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out our model using `print()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the more useful `summary()` (which we imported at the top of this notebook with `from torchsummary import summary`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (1,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we have two parameters? We have one for the weight (`w1`) and one for the bias (`w0`). These were initialized randomly by PyTorch when we created our model. They can be accessed with `model.state_dict()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, before we move on, the `x` and `y` data I created are currently NumPy arrays but they need to be PyTorch tensors. Let's convert them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = torch.tensor(X, dtype=torch.float32)  # I'll explain requires_grad next Chapter\n",
    "y_t = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a working model right now and could tell it to give us some output with this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model(X_t[0]).item()\n",
    "print(f\"Predicted: {y_p:.2f}\")\n",
    "print(f\"   Actual: {y[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction is pretty bad because our model is not trained/fitted yet! As we learned in the past few chapters, to fit our model we need:\n",
    "1. **a loss function** (called \"criterion\" in PyTorch) to tell us how good/bad our predictions are - we'll use **Mean Squared Error**, `torch.nn.MSELoss()`\n",
    "2. **an optimization algorithm** to help optimise model parameters - we'll use **Stochastic Gradient Descent**, `torch.optim.SGD()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.1\n",
    "criterion = nn.MSELoss()  # loss function\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)  # optimization algorithm is SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we train, we're going to create a \"data loader\" to help batch my data. We'll talk more about these in later chapters but just think of them as generators that yield data to us on request. We'll use a `BATCH_SIZE = 50` (which should give us 10 batches because we have 500 data points):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we should have 10 batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at a batch using this syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, yy = next(iter(dataloader))\n",
    "print(f\" Shape of feature data (X) in batch: {XX.shape}\")\n",
    "print(f\"Shape of response data (y) in batch: {yy.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data loader defined, let's train our simple network for 5 epochs of SGD!\n",
    "\n",
    "> I'll explain all the code here next chapter but scan throught it, it's not too hard to see what's going on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True):\n",
    "    \"\"\"Simple training wrapper for PyTorch network.\"\"\"\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        losses = 0\n",
    "        for X, y in dataloader:\n",
    "            # you code        # Clear gradients w.r.t. parameters\n",
    "            # you code       # Forward pass to get output\n",
    "            # you code        # Calculate loss\n",
    "            # you code         # Getting gradients w.r.t. parameters\n",
    "            # you code            # Update parameters\n",
    "            # you code       # Add loss for this batch to running total\n",
    "            \n",
    "            optimizer.zero_grad()       # Clear gradients w.r.t. parameters\n",
    "            y_hat = model(X).flatten()  # Forward pass to get output\n",
    "            loss = criterion(y_hat, y)  # Calculate loss\n",
    "            loss.backward()             # Getting gradients w.r.t. parameters\n",
    "            optimizer.step()            # Update parameters\n",
    "            losses += loss.item()       # Add loss for this batch to running total\n",
    "        if verbose: print(f\"epoch: {epoch + 1}, loss: {losses / len(dataloader):.4f}\")\n",
    "   \n",
    "trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model has been trained, our parameters should be different than before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the sklearn model, we get a very similar answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"w0\": [sk_model.intercept_, model.state_dict()['linear.bias'].item()],\n",
    "              \"w1\": [sk_model.coef_[0], model.state_dict()['linear.weight'].item()]},\n",
    "             index=['sklearn', 'pytorch']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got pretty close! We could do better by changing the number of epochs or the learning rate. So here is our simple network once again:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, check out what happens if we run `trainer()` again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model continues where we left off! This may or may not be what you want. We can start from scratch by re-making our `model` and `optimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Multiple Linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's do a multiple linear regression now with 3 features. So our network will look like this:\n",
    "\n",
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-3.png)\n",
    "\n",
    "Let's go ahead and create some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "X, y = make_regression(n_samples=500, n_features=3, random_state=0, noise=10.0) # sklearn\n",
    "X_t = torch.tensor(X, dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's create the above model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linearRegression(input_size=3, output_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have 4 parameters (3 weights and 1 bias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, (3,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good to me! Let's train the model and then compare it to sklearn's `LinearRegression()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_model = LinearRegression().fit(X, y)\n",
    "pd.DataFrame({\"w0\": [sk_model.intercept_, model.state_dict()['linear.bias'].item()],\n",
    "              \"w1\": [sk_model.coef_[0], model.state_dict()['linear.weight'][0, 0].item()],\n",
    "              \"w2\": [sk_model.coef_[1], model.state_dict()['linear.weight'][0, 1].item()],\n",
    "              \"w3\": [sk_model.coef_[2], model.state_dict()['linear.weight'][0, 2].item()]},\n",
    "             index=['sklearn', 'pytorch']).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Non-linear Regression with a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so we've made simple networks to imitate simple and multiple *linear* regression. You're probably thinking, so what? But we're getting to the good stuff I promise! For example, what happens when we have more complicated datasets like this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "np.random.seed(2020)\n",
    "X = np.sort(np.random.randn(500))\n",
    "y = X ** 2 + 15 * np.sin(X) **3\n",
    "X_t = torch.tensor(X[:, None], dtype=torch.float32)\n",
    "y_t = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create dataloader\n",
    "dataset = TensorDataset(X_t, y_t)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.5, label='Data Points')\n",
    "plt.title('Regression Data', fontsize=14)\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.ylim([-25,25])\n",
    "plt.xlim([-3,3])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is obviously non-linear, and we need to introduce some **non-linearities** into our network. These non-linearities are what make neural networks so powerful and they are called **\"activation functions\"**. We are going to create a new model class that includes a non-linearity - a sigmoid function:\n",
    "\n",
    "$$S(X)=\\frac{1}{1+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Generate a range of x values\n",
    "x_range = np.linspace(-10, 10, 500)  # Covers a wide range to show the curve\n",
    "sigmoid_values = sigmoid(x_range)\n",
    "\n",
    "# Plot the sigmoid curve\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.plot(x_range, sigmoid_values, label=\"Sigmoid Curve\", color=\"green\", linewidth=2)\n",
    "\n",
    "# Adding labels and title\n",
    "# plt.axvline(0, color='gray', linestyle='--', alpha=0.7)  # Highlight x=0 (decision boundary)\n",
    "# plt.axhline(0.5, color='gray', linestyle='--', alpha=0.7)  # Highlight y=0.5\n",
    "plt.xlabel(\"Input Values (x)\")\n",
    "plt.ylabel(\"Sigmoid Output (S(x))\")\n",
    "plt.title(\"Sigmoid Function Curve\")\n",
    "plt.grid(alpha=0.4)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# xs = np.linspace(-15, 15, 100)\n",
    "# plot_regression(xs, [0], sigmoid(xs), x_range=[-5, 5], y_range=[0, 1], dy=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll talk more about activation functions later, but note how the sigmoid function non-linearly maps `x` to a value between 0 and 1. Okay, so let's create the following network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/Shangyue-CWU/CS457Draft/refs/heads/main/img/nn-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All this means is that the value of each node in the hidden layer will be transformed by the \"activation function\", thus introducing non-linear elements to our model! There's two main ways of creating the above model in PyTorch, I'll show you both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonlinRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)       # input -> hidden layer\n",
    "        x = self.sigmoid(x)      # sigmoid activation function in hidden layer\n",
    "        x = self.output(x)       # hidden -> output layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how our `forward()` method now passes `x` through the `nn.Sigmoid()` function after the hidden layer. The above method is very clear and flexible, but I prefer using `nn.Sequential()` to combine my layers together in the constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nonlinRegression(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.main = torch.nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),  # input -> hidden layer\n",
    "            nn.Sigmoid(),                        # sigmoid activation function in hidden layer\n",
    "            nn.Linear(hidden_size, output_size)  # hidden -> output layer\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an instance of our new class and confirm it has 10 parameters (6 weights + 4 biases):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nonlinRegression(1, 3, 1)\n",
    "summary(model, (1,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # Mean Squared Error (MSE) Loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.3) # Stochastic Gradient Descent (SGD) lr (learning rate) = 0.3\n",
    "\n",
    "# Call the training function to train the model\n",
    "# - model: The neural network to train\n",
    "# - criterion: The loss function used to calculate the error\n",
    "# - optimizer: The optimization algorithm to update model parameters\n",
    "# - dataloader: Provides batches of data for training\n",
    "# - epochs: Number of times the entire dataset is passed through the model\n",
    "# - verbose: If True, provides detailed output during training (e.g., loss values after each epoch)\n",
    "trainer(model, criterion, optimizer, dataloader, epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_p = model(X_t).detach().numpy().squeeze()\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(X, y, color='blue', alpha=0.5, label='Data Points')\n",
    "plt.plot(X, y_p, color='red', label='Fitted line')\n",
    "plt.title('<Your name> + Regression Data', fontsize=14)\n",
    "plt.xlabel('Feature (X)', fontsize=12)\n",
    "plt.ylabel('Target (y)', fontsize=12)\n",
    "plt.ylim([-25,25])\n",
    "plt.xlim([-3,3])\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please run y_p and model(X_t) explain why we do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
